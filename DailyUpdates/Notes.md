# 10/3/2024

* Triton Study
* FineTune llama3.2 model with different techniques
  * Unsloth designed to optimize the training and fine-tuning of LLMs
* Exercise for an hour

## Key Features of OpenMP:

Parallelism: Allows you to parallelize code by using compiler directives, library routines, and environment variables.
Shared Memory Model: Uses a shared memory model where multiple threads can access shared data.
Scalability: Scales from a few threads on a single processor to thousands of threads on a multi-processor system.
Ease of Use: Simplifies the process of parallel programming by allowing incremental parallelization of existing code.

OpenMP is widely used in scientific computing, engineering simulations, data analysis, and any application that can benefit from parallel processing.

## What is PrimTorch?

PrimTorch is a project within the PyTorch ecosystem that focuses on providing a set of primitive operations for deep learning. These primitives are low-level operations
that serve as building blocks for more complex neural network operations. The goal of PrimTorch is to offer a minimal and efficient set of operations that can be used
to implement higher-level functionalities in a flexible and optimized manner.

Key Features of PrimTorch:
Low-Level Primitives: Provides a set of basic operations that can be combined to create more complex neural network layers and functions.
Optimization: Designed to be highly efficient, making it suitable for performance-critical applications.
Flexibility: Allows developers to build custom operations and layers by combining these primitives.
Integration: Seamlessly integrates with the broader PyTorch framework, enabling easy use alongside other PyTorch components.
Example Usage:
While specific examples of PrimTorch usage might vary, the general idea is to use these primitives to construct custom neural network components. Here is a conceptual example:

Applications:
PrimTorch is particularly useful for:

Custom Layer Development: Creating custom neural network layers that require specific low-level operations.
Optimization: Fine-tuning performance-critical parts of a neural network.
Research: Experimenting with new neural network architectures and operations.

**dynamic ** specifies whether to enable the code path for Dynamic Shapes. Certain compiler optimizations cannot be applied to dynamic shaped programs

Compile Mode vs Eager Mode

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

# 10/4/2024
